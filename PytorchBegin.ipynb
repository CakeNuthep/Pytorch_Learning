{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOgG7GSoWBPv+tFrckeg29H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CakeNuthep/Pytorch_Learning/blob/main/PytorchBegin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKkl5JUu11vM"
      },
      "source": [
        "# การติดตั้ง"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4-Aj9LuvVaJ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyJ1JUvDwLtF",
        "outputId": "6a83ec94-1675-4a5f-af8a-97dcd1eada90"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxAOW7lwddy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a8a86a-f56e-4394-dd13-5c9206c28090"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiZhg11w16cu"
      },
      "source": [
        "# การใช้ง่าน Tensor ใน PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBryesynxAeE",
        "outputId": "0525b8a5-a92e-4530-d647-4c53106f2e5e"
      },
      "source": [
        "x = torch.rand(3,4,dtype = torch.float32)\r\n",
        "dtype = print(x)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8550, 0.3836, 0.7550, 0.5535],\n",
            "        [0.8444, 0.7391, 0.7754, 0.2669],\n",
            "        [0.9069, 0.4431, 0.7708, 0.2169]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKGSrDDnxKLf",
        "outputId": "f3799edb-4f14-4a94-aa84-0167e6dcb0fb"
      },
      "source": [
        "x = torch.zeros(5,3, dtype= torch.long)\r\n",
        "print(x)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40-v1QUQd88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a889058-9dd5-4dda-f3cb-2e33be5db748"
      },
      "source": [
        "x = x.new_ones(5,3,dtype=torch.float64)\r\n",
        "print(x)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT5oOGYbxX3O",
        "outputId": "33d7b5b5-2f4d-4fbd-c95f-76187f7e4b50"
      },
      "source": [
        "x = torch.tensor([4.123,3])\r\n",
        "print(x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4.1230, 3.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rGxogQNxvXu",
        "outputId": "3f8a3d12-b0a7-42c1-fa70-0d86557a0f95"
      },
      "source": [
        "x = torch.randn_like(x, dtype=torch.float32)   #result has the same size\r\n",
        "print(x)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.5789,  0.5610])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuL65E5AeKL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643f2f9e-7d97-4125-9759-b7f220509184"
      },
      "source": [
        "x = torch.Tensor(5,3).fill_(7)\r\n",
        "print(x)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[7., 7., 7.],\n",
            "        [7., 7., 7.],\n",
            "        [7., 7., 7.],\n",
            "        [7., 7., 7.],\n",
            "        [7., 7., 7.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOmJDnnpeYqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e3d396-246c-4d4e-a14d-25759e855fa1"
      },
      "source": [
        "x = torch.Tensor(5,3).fill_(7).type(torch.int)\r\n",
        "print(x)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[7, 7, 7],\n",
            "        [7, 7, 7],\n",
            "        [7, 7, 7],\n",
            "        [7, 7, 7],\n",
            "        [7, 7, 7]], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mxaAJUb05z5",
        "outputId": "3dc2260a-1539-4ec3-f0d6-c4f4272a610b"
      },
      "source": [
        "torch.manual_seed(1)\r\n",
        "x = torch.randn(5,3)\r\n",
        "print(x)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6614,  0.2669,  0.0617],\n",
            "        [ 0.6213, -0.4519, -0.1661],\n",
            "        [-1.5228,  0.3817, -1.0276],\n",
            "        [-0.5631, -0.8923, -0.0583],\n",
            "        [-0.1955, -0.9656,  0.4224]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfF5dOg12MVp"
      },
      "source": [
        "# การแปลงไปมาระหว่าง numpy กับ Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM6Xz5CB2ZXy",
        "outputId": "785c5995-c49b-4dee-a1e6-9791cc0de16a"
      },
      "source": [
        "import numpy as np\r\n",
        "a = np.array([1.,3.])\r\n",
        "t = torch.from_numpy(a)\r\n",
        "print(t) # ได้ tensor([1.,3.])\r\n",
        "print(a) # ได้ [ 1.,3.]\r\n",
        "t += 1\r\n",
        "print(t) # ได้ tensor([2.,4.])\r\n",
        "print(a) # ได้ [ 2.,4.]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 3.], dtype=torch.float64)\n",
            "[1. 3.]\n",
            "tensor([2., 4.], dtype=torch.float64)\n",
            "[2. 4.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqt8UMsF3hEP",
        "outputId": "3e376e1a-70dc-4f28-c519-7e18a5b5a6f5"
      },
      "source": [
        "a = np.array([1.,3.])\r\n",
        "t = torch.Tensor(a)\r\n",
        "print(t) # ได้ tensor([1.,3.])\r\n",
        "print(a) # ได้ [ 1.,3.]\r\n",
        "t += 1\r\n",
        "print(t) # ได้ tensor([2.,4.])\r\n",
        "print(a) # ได้ [ 1.,3.]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 3.])\n",
            "[1. 3.]\n",
            "tensor([2., 4.])\n",
            "[1. 3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WSIORCE2kYu",
        "outputId": "fdfc5e28-ae3d-4ff7-c389-bb9309d161dd"
      },
      "source": [
        "t = torch.Tensor([2,3])\r\n",
        "b = t.numpy()\r\n",
        "print(b) # ได้ [ 2.,3.]\r\n",
        "b += 1\r\n",
        "print(b) # ได้ [ 3.,4.]\r\n",
        "print(t) # ได้ tensor([3.,4.], dtype=torch.float64)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2. 3.]\n",
            "[3. 4.]\n",
            "tensor([3., 4.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnYHQrI73nZJ",
        "outputId": "63f32e90-ae0f-466d-fc0a-80290b852fea"
      },
      "source": [
        "t = torch.Tensor([2,3])\r\n",
        "b = np.array(t.data)\r\n",
        "b += 1\r\n",
        "print(b) # ได้ [ 3.  4.]\r\n",
        "print(t) # ได้ tensor([2., 3.])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3. 4.]\n",
            "tensor([2., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ipTQ9Y2M5w"
      },
      "source": [
        "# การคำนวณ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAdPUze1yonH",
        "outputId": "68ff0c96-4637-417a-e6d0-36da83cc45ec"
      },
      "source": [
        "t1 = torch.ones(5,3)\r\n",
        "print(\"t1\")\r\n",
        "print(t1)\r\n",
        "\r\n",
        "t2 = torch.zeros(5,3)\r\n",
        "print(\"t2\")\r\n",
        "print(t2)\r\n",
        "\r\n",
        "print(\"t1+t2\")\r\n",
        "print(t1+t2)\r\n",
        "\r\n",
        "print(\"t1-t2\")\r\n",
        "print(t1-t2)\r\n",
        "\r\n",
        "print(\"t1*t2\")\r\n",
        "print(t1*t2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t1\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "t2\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "t1+t2\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "t1-t2\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "t1*t2\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9B7MUF96J55"
      },
      "source": [
        "## Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvV8J24W6JX3",
        "outputId": "739ca9b5-306e-4f33-d0bb-1c5181bc9c0e"
      },
      "source": [
        "t1 = torch.ones(5,3)\r\n",
        "t2 = torch.Tensor(3,5).fill_(2)\r\n",
        "print(\"t1\")\r\n",
        "print(t1)\r\n",
        "\r\n",
        "print(\"t2\")\r\n",
        "print(t2)\r\n",
        "\r\n",
        "print(\"dot product\")\r\n",
        "print(torch.matmul(t1,t2))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t1\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "t2\n",
            "tensor([[2., 2., 2., 2., 2.],\n",
            "        [2., 2., 2., 2., 2.],\n",
            "        [2., 2., 2., 2., 2.]])\n",
            "dot product\n",
            "tensor([[6., 6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6., 6.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Cjh8W97G8M",
        "outputId": "7c206ebd-473e-4eb3-8cf8-8bd6afe76dd0"
      },
      "source": [
        "t = torch.ones(5,3)\r\n",
        "print(t.mean()) # ได้ tensor(1.)\r\n",
        "print(t.sum()) # ได้ tensor(15.)\r\n",
        "print(t.std()) # ได้ tensor(0.)\r\n",
        "print(t.max())\r\n",
        "print(t.min())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.)\n",
            "tensor(15.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxR94V7T71Xp"
      },
      "source": [
        "# การเปลี่ยนรูปร่าง"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZmWVhW07Nhs",
        "outputId": "b397605f-236d-4868-fb4b-9eec57871567"
      },
      "source": [
        "t = torch.ones(5,3)\r\n",
        "print(t)\r\n",
        "print(t.view(1,15))\r\n",
        "print(t.view(3,5))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMNmLaCe8Ugx",
        "outputId": "96093da3-f49e-403e-e454-52c87bcb11da"
      },
      "source": [
        "print(t.flatten())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP0JEw45_SGR"
      },
      "source": [
        "# การใช้ GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHnFtLFt_fBu"
      },
      "source": [
        "device = torch.device(\"cuda\")\r\n",
        "t = torch.ones(5,3).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9AHuUCUAAGO"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqzvM7dq6PGp"
      },
      "source": [
        "## อนุพันธ์"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njTNE4EF9HPk",
        "outputId": "7d2713c2-7e62-471b-e7cd-009bb19a1c9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.ones(1,5)\r\n",
        "w = torch.Tensor(5,1).fill_(2)\r\n",
        "\r\n",
        "w.requires_grad = True\r\n",
        "y = torch.matmul(x,w)\r\n",
        "\r\n",
        "y.backward()\r\n",
        "print(y.grad)\r\n",
        "print(w.grad)\r\n",
        "print(x.grad)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DukTG1z7z1Vh"
      },
      "source": [
        "## การสร้าง Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfMLnSZy_g3T"
      },
      "source": [
        "linear_layer = torch.nn.Linear(2,3)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVvr06_H1RPj",
        "outputId": "835dd11a-2086-4c53-fcb4-67f2d3f79053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "linear_layer.weight"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.4667, -0.6443],\n",
              "        [-0.6723, -0.3411],\n",
              "        [ 0.6209, -0.1178]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89MdDX-kml46"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeBm_MsZmkyK",
        "outputId": "d6f14537-24cd-4743-df57-707767cb4ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# จำลองตัวอย่างข้อมูล \r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "x = np.random.uniform(0,1,100) #สุ่มค่า 0 ถึง 1 มา 10 จุด\r\n",
        "y = x*0.8+np.random.normal(0.5,0.05,100)\r\n",
        "\r\n",
        "plt.scatter(x,y,c='b',edgecolor='r') #วาดกราฟ"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f7a5406a5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAekUlEQVR4nO3df2wc93nn8fezpCyVFBWrlBjbkUkqgltQUg5IzCuSg9BKSNoawkH2JUahFeU0qWLBSs3LocU1CbaHVAnYux5wRVLVaSpHruJIpJs0PpdoE+SQWIJBN05Dw0mkWEhkWT+iWI1k2aIksvrB5ff+2KW8JHd2Z3dndnZmPy9gIe7u7Mx3TPnZr555vs+Ycw4REYm/VNQDEBGRYCigi4gkhAK6iEhCKKCLiCSEArqISEK0RnXgFStWuN7e3qgOLyISSy+++OLrzrmVxd6LLKD39vYyPj4e1eFFRGLJzE57vVc25WJmT5jZeTM7Wma7/2hm02b2YDWDFBGR2vjJoe8H7iu1gZm1AH8B/L8AxiQiIlUoG9Cdc88Bb5TZbBD4BnA+iEGJiEjlaq5yMbN3AP8F+Bsf2+40s3EzG79w4UKthxYRkQJBlC1+Hvikc26m3IbOub3OuX7nXP/KlUUv0oqISJWCCOj9wFNmdgp4EPiimT0QwH5FRBJl5uAIV3rXM5Nqyf15cCTQ/ddctuicWz37s5ntB/7JOfdMrfsVEUmSmYMjnN+ZIT21jzE2sOH0GCM7d9AFpAbSgRzDT9niCPA94NfN7KyZ7TCzR8zskUBGICLSBCYzQ6Sn9nGYTUyziMNsIj21j8nMUGDHsKj6off39zstLBKRZjGTamGxu8Y0i2691spNrtsSUjNZ3/sxsxedc/3F3lMvFxGROpjs7mMDY3Ne28AYk919gR1DAV1EEinsC5CVHrd9KMNI2w42cohWbrKRQ4y07aB9KBPcwZ1zkTzuvfdeJyIShuyBYXeubbXbyLOulRtuI8+6c22rXfbAcKTHzR4Ydpd71rmspXJ/VjEeYNx5xFXl0EUkca70rmfL6T0cZtOt1zZyiNGeQTpOlWxL1fDHLZVDV0AXkcQJ6gJkIx5XF0VFpKnU4wJkIx13lgK6iCROXS5ANtBxZ0V2gwsRkbCkBtJ0AaOZQdrPHGOyu4/2oaHAVmQ22nFnKYcuIhIjyqGLiDQBBXQRkYRQQBcRSQgFdBGRhFBAFxFJCAV0EZGEUEAXEUkIBXQRkYRQQBcRSQgFdBGRhFBAFxFJCAV0EZGQ1Ps2eAroIiIhmDk4wvmdGbac3sNid40tp/dwfmcm1KCugC4iEoLJzBDpqX0cZhPTLOIwm0hP7WMyMxTaMRXQRURC0H7mGGNsmPPaGBtoP3MstGMqoIuIhCCK29HpjkUiIiFoH8rw9Ee3cvFmB6s5yUlW07noCu1Dnw/tmJqhi4iE5Lot4WEeZwnXeJjHuW5LQj2eArqISAgmM0Okb+yfe1H0xn5dFBURiRtdFBURSYgoLooqoIuIhKB9KMNI2w42cohWbrKRQ4y07aB9KBPaMVXlIiISgtRAmi5gNDNI+5ljTHb30T40RGogHdoxywZ0M3sC+M/Aeefc+iLvDwCfBAy4Auxyzv0o6IGKiMRNaiBNRz6Ad9TjeD622Q/cV+L9k8BvOefeBXwO2BvAuEREpEJlZ+jOuefMrLfE+/9S8PQFYFXtwxIRkUoFfVF0B/CtgPcpIiI+BBbQzWwTuYD+yRLb7DSzcTMbv3DhQlCHFpE6CrLHd737hSddIAHdzP4D8GXgfufcRa/tnHN7nXP9zrn+lStXBnFoEakjvz2+/QTqKPqFJ55zruwD6AWOerzXDbwC/Cc/+5p93HvvvU5E4uVyzzq3kWcduFuPjTzrLvesu7VN9sCwO9e22m3kWdfKDbeRZ925ttUue2C44n3JQsC484irlnvfm5mNABuBFcAvgc8Ai/JfBl8ysy8DHwJO5z8y7ZzrL/dF0t/f78bHxyv+AhKR6MykWljsrjGdCwEAtHKT67aE1EwWgCu969lyeg+H2XRrm40cYrRnkI5TRyvaV8mxHBxhMjNUUOOdCbXGu1GY2YteMdZPlUvJ/0LOuY8BH6tybCISI5PdfWw4PTYnWM8uZ5+ts/bbw8TPvrzMpmvSU/sYYwMbTo8xsnMHXdAUQd2Llv6LiG9+lrP76WEyc3CElqsTjJCuaml8FLd3iwWvXEzYD+XQReIpe2DYXe5Z57KWyv05LzdeLode+P42nnTHWeOymJvo7F2wL88xWMq1cmNO/r2VGy5rqcDPt9FQIoeugC4ivpQL5H63DeJiaDNfUFVAF5Ga+K1c8bWvAGbX1Yynki+kRqaALiI18ZoRT3T2Vhwkg5pdV/ovhqC+kKKmgC4iNfGcVWMVB8kogmuSUjSlAnrZOvSwqA5dJD68assf52Hu4ZU5r82vNy+m3jXktda8N5JSdegqWxSRsoqWK5LmM3xmznZ+75mZGkjTceooqZls7s8iwTzIPi9R3A4uCgroIlJWaiBN194hRnsGuW5LGO0ZZFnnIl6b1y07qCAZdJ+XKG4HFwmvXEzYD+XQReItzFx4GDnvZqhyUQ5dRKoWVi48STnvoCmHLiKh8JMLr0az5LyDpoAuIg2naXLeAVNAFxFf6nl3oWIXYbv2DjV1J0U/FNBFpKxyVSdhBPuw0jlJVrYfuojItU/8CVenFvMdPsAvuItpWlk5dYYrv/+HLH3+X7jwlX9Wb/IGoBm6iJQ0c3CEyxdv8jCP82H2M0OKHTzBEq7xQPYbXPjSN9g7tU29yRuAyhZFpKTCZf9HWM8gC1sA7GGQd/HWcn+VGIZHZYsiUrXCW8r1Ufz2cn3MXe6vEsNoKKCLSEmFNeHHKF4ffsWWFS0xnDk4wtSKu7lsHcxYissrVodaHdPsFNBFpKTCmvD/ySf5Oz6yIHh3PLJ9QYkhwMRH/xvnL6a4n1EWc537Lz7B+T/4lIJ6WLx6AoT9UC8XkdrUszdJ4bEmO1fl7v9Z5riXe9a546xJTB/yRoF6uYgky2xd+K1SQcYYadvRUItvZlItOAdLUE+WIOmiqEjCTGaGSE/ta+hSwcnuPk6yWj1Z6kgBXSSGCitPZvm9uUS9tA9l6Fx0ZWHO/baPqCdLSBTQRWIoDt0IUwNp3vZ3n6erc4Z/ZAvXWcw/dv4BXU/8r4ZJCyWNArpIDEXVjbDSni2pgTRtr/+cZe4KKTfDstdPKpiHSAFdJIb8dCOspWFWsc8GfVs4CYFX+UvYD5UtigRnfgnj9K5H3bm21W43GXeEdW6alHvTbnfTux71ta/CW8vtJuPetNtdFnPHWeO2MqwSxAhRomxRAV0kAkHWkBe9t6fd6b7OA+4EC18vd6zC+3luZXjBPk6w+lZQb+WGy1qq6rFL5RTQRRpIrTdXnv9lMNm5qujinddZXtWinqylXCs3HDh3gt6i+zjCOs3QI6KALtJAarmjfdEvA97utvHknP21csNlsVuBec7rZWbUs+PbyrDnPqZJVfxFJMFQQBdpIIUz4EoCrXPeXwbHWbPgtUssq26Gnv/SOM4az6X7Eyx1l1qW+8rJS7BqCujAE8B54KjH+wb8FfAK8GPgPeX26RTQpYnVNEP3+jLAFqRwbr7/d905u7Oq1E72QG52vo0nF+bh8/8i0Aw9GrUG9N8E3lMioG8GvpUP7O8Fvl9un04BXZpYLTl0ry+Dic7eohdZa7n4Wph6ma2UOc4ad5pVqnKJUM0pF6C3RED/WyBd8PynwJ3l9qmALs2s2kBb6wXVSsc4/1iFFS6qcolG2AH9n4ANBc+/C/R7bLsTGAfGu7u763T6IvVRr3a2UbXNvdSy3O0mo1a4EWuYgF740AxdkqSeM+eoNMM5xkGpgB7E0v9fAHcXPF+Vf02kacShnW2t/LQbkGgFEdBHgQ9bznuBCefcuQD2KxKJanqgNEI721p6t/iVGkjTceooqZls7k8F84ZSNqCb2QjwPeDXzeysme0ws0fM7JH8Jt8EXiVXtvg48PHQRisSsmobUNXazrbWYKzGWQJoYZFIoWprxGvJLweRm66ltl3iBd1TVMSfmVQLi11198CcOTjCZGaI9jPHmOzuo30o4yslcaV3PVtO7+Ewm269tpFDjPYM0nHqaOjjlnjRPUVFfKoldVJtfjmI/Hsc7mAk4VNAFykQxZ2AggjGUd3BSBqMVy4m7Idy6NKo6rlwZ/Z4QdR313vcEg2UQxdpbNXm36X5KIcu0qBmyxV5aHvuha8eUH23VE0BXSQiqh2XoCmgi0SkGdoFSH0poItEpBHaBUiyKKBLU6pH35NyVDsuQVNAl8TyCtqNkrtW7bgEzqueMeyH6tAlTKVquxup74lqx6VSqA5dmk2p/ijtZ44V7XtyjcVM9axVLbg0NNWhS9MpdcGxWO76f7CbC3ZH5GkYkVoooEsilbrgWCx3/V/tMdLuoEoIJdYU0CWRSl1wLHYrtWXuskoIJfZaox6ASBhSA2m6gNHMYEFO3Pv+l9c672LDxbE5OffZGX1HncYsUivN0CWWytWRl2p2Vaxs8fKVFE8v2spu/pQjrGeaFv6vfZC2zZuKHV6kMXmVv4T9UNmiVKtcu9ly73uVLV5tX+nOccfcz3GHm971aJSnKzIHJcoWFdAldsrVkZd7P2sp18qNOe+3csNNsLTo597kdtWHS8MoFdCVcpHYKdcDpdz7XhUwS5ks+rkOLhetdmmE9gEihRTQJXa8AvK1X72LK73rueraSvZI8aqAuUp70c8do4/203OrXRqlfYDIHF5T97AfSrlItYrlyN9Y1OXO3dbtNvKs28aT7iTdJW/plj0w7CY6e10WcxMsdZOdq9zV9pXuNd4+53MnWO12k3GXWpbPGUMjtQ+Q5kKJlIvKFiV2ipUkuqttpC8+cavscIZWHudh3smrTPasLVqyOPXvxv18lzE2sOHiGE8v2spKzvM0H2QZlzlGHwfYxnaGWZq9NOezan0rjUgpF4ml1EA6d6u2mSwdp46y9I0zcwLsU6Tp4xiYFb2l27VP/AlXp1J8hw/wEu/mDv6ND958isnUMvbwhxyjjz6O8UGe4QDbmOpZO+fzan0rjUgBXRKhkgA7c3CEyxdv8jCPs4RrDLKHITLcxVmWzlxmZ9swg+y59d7OtuEFLW3V+lYaklcuJuyHcugyK4gWsuVqzwt55b+Ps+bW8f2MR61vJQqoDl0aVSWB2M++fAVijzr0LKagLA2vVEBXykUiVc2Nkr3qv+fn1b36tnilZ6529qj/ucSaArpEqtJqkSDqv73y30u/8Oc1nYtI1BTQJVKVVotUM6Ofr1j73K693p0YReJCt6CTSM3OuNNT+3L14Iwx0rbDM8DOpFqK3j7uui0hNZOt59BFIlHqFnRaWCSRqrRv+WR3HxtOq2+5SDG+Ui5mdp+Z/dTMXjGzTxV5v9vMDpnZS2b2YzPbHPxQJan8XswE1X+LlFJ2hm5mLcBjwG8DZ4EfmNmoc+7lgs3+FPiac+5vzGwt8E2gN4TxSpOrdEYv0kz8zNB/A3jFOfeqc+4G8BRw/7xtHLAs//PbgNeCG6IkUS2tZyuZ0avFrTQTPwH9HcDPC56fzb9W6M+A7WZ2ltzsfLDYjsxsp5mNm9n4hQsXqhiuNIJag2S9Ws8WHucht59fnr4G2we4vGK1Arskk9eKo9kH8CDw5YLnDwF/PW+bPwL+OP/z+4CXgVSp/WqlaDwFsbKzXq1nZ4+zlWF3gmBWo4pEjVqW/ucD9LcLnn8a+PS8bX4C3F3w/FWgq9R+FdDjyU8wLrcE33PpvaUCHevscY6g3uWSHKUCup+Uyw+Ae8xstZndBmwFRudtcwZ4P4CZ9QFLAOVUGkwQ+eRyKzv9pFPq1Xp29jh9qHe5NAmvSO/mzsA3Az8DTgCZ/GufBbbkf14LPA/8CPgh8Dvl9qkZen2VSpVM73rUXWpZ7rKYu9SyvORd7mu9QXO5sYRxzsdZoxm6JAbqtihegTZ327U75gTX17jDM6iXC8Z+0yn1aj2bPTDsJjtXuXPzbi2nHLrElQJ6kyoMmlmsaKCdYGnRQD//Hppe+50fjIO64Bl0wFfvckkKBfQmNH8m7ZV28Ar0WczXMeYHyeldj7pzdmdNs+F6pWRE4kgBvQnNnylvZdidpHtBauVN3lbxDN254kH3jUVd7txt3W43GXeEdW6alHvTbi+Zk/czduW8Rd6igN6EiuWyt/Gkm2Cpy1rKXWpZ7naTcV/g0Ypy6LOKBd2gLj7Wq6xRJI5KBXT1Q29gtZQZFisNfI1VWE9Pbsn8zASf4zN8gj18nQf5Bz7EdRbzDA/QtetBWr64p+Q4ipUvruZkIOWB9SprFEkcr0gf9kMz9NKqzSPfymvn0x27yRT9vN+0htc4JjtXhTdDVw5dxBNKucRPNXnkooHQ7nTT2ILKDr9B02scE529njn0et7wWaTZKKDHUDV55Eq/BMou0T8w7LKY28aTty5yHmGd28aTuVLIIp9XIBYJlwJ6DFU1Qw/wYuLsDP413r6gOuYk3W6yc5Xv/SjAiwSnVEDXRVGf6t1Xu5I788yO7aprq+hiYqlzmr0Z87/zK3yU/XNuyvxR9jPt4+6F9WqTKyJ5XpE+7EecZuhRXaQrnN1OdPa6yc5VC2a6hWPbxpMLZtNe4/S7hH+a6mf9qicXCR5KudQm6sBUKvgWW0B0nDUu63EhdPYL4lLLcl9NtmppPat6cpHgKaDXKOrAVOoLpZJmWIVfCuVm3rPb7yZT9c0hov4iFEkiBfQaRR2YCoP2VoZvVZxMsLRoPXixsc0/B6+Z90Rn74I0z3S+rW6Wyi5sqp5cJHgK6DWKOjCVvJXabd3ujUVdZcc2fyZfy74qoSoXkWApoAcgysBU7kYNhbNqr7EV+1fGbjK5mXf+c35n+yISnVIB3XLv119/f78bHx+P5NhxNHNwBLYPsJjrTLPo1uut3OS6LSE1ky37+fM7M6Sn9jHGBjYwxkjbDrr2DpEaSOe2SbWw2F2rav8iUh9m9qJzrr/Ye6pDz6t3nXmlx0sNpJnsWVt106rUQJquvUOM9gxy3ZYw2jM4J5iDmmKJxJ7X1D3sRyOlXOqdI6+l8Valn6skVRT1tQIRKQ/l0EurdxVLLccLO0DrIqZIYysV0JVDp/rc8czBESYzQ7SfOcZkdx/tQ5k5KYygj1epK73r2XJ6D4fZdOu1jRxitGeQjlNHAzuOiNSPcuhlVJM7LtWnpFx+vF656mI3oajmhhMiEhNeU/ewH42UcqkmNeGVNpnsXFV2X/XKVUe9IEpEgody6OVVmjv2WnI/wVLfdwIKO1eti5wiyVMqoCuHXiWv/PR3eX/VteJhqDbPLyKNSTn0EHj1K7/a2RNJLff8vH3244Nc6V0PD23PbfDVA3ScOqpgLpJkXlP3sB+NlnKphtct2Oqd5ph/zN1k3GvcoVSLSAKhlEt91TvNMT/9c4T1DKJyRZEkKpVyUUBPgPl17dO0sAT1ZBFJIuXQE25+Xfsx1JNFpBkpoCfA/Au0T/MAw2zzdYNpEUmO8rdul4aXGkjTBYxmBm/l7ds2P8joNwcL8vhDqnARSThfM3Qzu8/Mfmpmr5jZpzy2+T0ze9nMfmJmw8EOs37q3UY3qGOmBtK5ssSZLB2njtLyxT1znnsF8yjOV0RC4lX+MvsAWoATwDuB24AfAWvnbXMP8BKwPP+8q9x+G7FsMcySQ6+VoVGu5tRKUpH4oZal/8D7gG8XPP808Ol52/xv4GPl9uUaPKCH1fukVOCMst+Ker2IxE+tAf1B4MsFzx8C/nreNs/kg/rzwAvAfR772gmMA+Pd3d31+y/gk1d/lqylatpvqcBZ6phh93sJ63xFJDylAnpQVS6t+bTLRiANPG5mt8/fyDm31znX75zrX7lyZUCHDk5YbW1LtbH1Oua1X73Lsz1vUHTLOZFk8RPQfwHcXfB8Vf61QmeBUefcTefcSeBn5AJ8rHj1Z6m13K9U4PQ65jStpKf2cZhNTLOIw2wiPbWPycxQ0WNUc3EzrPMVkYh4Td3dW2mSVuBVYDVvXRRdN2+b+4Cv5H9eAfwc6Cy130bMoTsXTlvbchcfi/aEqSAdUsvFTd1yTiReqLUfOrCZ3Kz7BJDJv/ZZYEv+ZwP+EngZOAJsLbfPRg3oYak0cFZywVIXN0WaR6mArl4uDWr2FnfpqX2MsYENjDHStoOuvQsXCNXrHqUiEr1SvVy0UrRBFVv96bXac7K7jw2nx+Z0V5zN0XfUccwiEi31cmlg81d/eq321MVNEYEEBHQtXc/P5vcOMdozyHVbwmjPYNHUjIgkW6xTLgvyzKfHGNm5gy5oumCWGkjTkT9npVlEmlOsZ+iTmaGKarVL0UxfROIu1gG91ArMSszO9MNclSkiErZYB/Sglq5XOtPXbF5EGlGsAvr8QNq2eVMg1R2VzPQ1mxeRhuW14ijsR6UrRb2Wt0/verTmpetalSkicUEdui2GbjYtcgf/xku8m+/wAa5Opbj+tWd81WqDd6qkkjruoPL2IiJBi01Abz9zjLs4yxAZBtnDEq7xMI9z+eLNoumO+cE7+/FBz1RJJXXcajkrIg3La+oe9qPSlMvlnnXuOGt8pTuKpWfetNsDSZXotm0iEiWS0Jxr5uAIbB9gMdfLNqG60rueLaf3zOltMk0LSwimgdXMwREmM0MFPVYyTbeQSUSiUao5V2xSLqmBNFc7e3ylO4rluY8RXKrEb48VEZF6ik1AB1j6hT/3dfGyWJ77aR5gxAbUwEpEEitWAd3vxctiVSs724ZZ+ciH1MBKRBIrNjn0SinPLSJJ1JQ3uFD3QRFpNrFKuXhRbxURkQQEdPVWERHJiX1AD7InuohInMU+oKu3iohITuwDunqriIjkxD6g6473IiI5sS9bTA2k6QJGM4MFNedaMCQizSf2AR1Ucy4iAglIuYiISI4CuohIQiQ6oGsFqYg0k8QGdK0gFZFmk9iArhWkItJsEhvQtYJURJpNYgO6VpCKSLPxFdDN7D4z+6mZvWJmnyqx3YfMzJlZ0ebr9aQVpCLSbMouLDKzFuAx4LeBs8APzGzUOffyvO06gE8A3w9joJXSClIRaTZ+Vor+BvCKc+5VADN7CrgfeHnedp8D/gL474GOsAZaQSoizcRPyuUdwM8Lnp/Nv3aLmb0HuNs598+ldmRmO81s3MzGL1y4UPFgRUTEW80XRc0sBfwl8MfltnXO7XXO9Tvn+leuXFnroUVEpICfgP4L4O6C56vyr83qANYDh83sFPBeYLQRLoyKiDQTPwH9B8A9ZrbazG4DtgKjs2865yaccyucc73OuV7gBWCLc248lBGLiEhRZQO6c24aeBT4NnAM+Jpz7idm9lkz2xL2AEVExB9zzkVzYLMLwOkqP74CeD3A4cRFM563zrl5NON5V3POPc65ohchIwvotTCzcedc0+Xom/G8dc7NoxnPO+hzTuzSfxGRZqOALiKSEHEN6HujHkBEmvG8dc7NoxnPO9BzjmUOXUREForrDF1EROZRQBcRSYiGDujl+rCb2WIz+/v8+983s976jzJYPs75j8zsZTP7sZl918x6ohhn0OLYc79Wfs7ZzH4v//v+iZkN13uMQfPx97vbzA6Z2Uv5v+OboxhnkMzsCTM7b2ZHPd43M/ur/H+TH+ebHVbHOdeQD6AFOAG8E7gN+BGwdt42Hwe+lP95K/D3UY+7Due8CWjL/7wr7ufs97zz23UAz5FrL9Ef9bjr8Lu+B3gJWJ5/3hX1uOtwznuBXfmf1wKnoh53AOf9m8B7gKMe728GvgUYuV5Y36/2WI08Q7/Vh905dwOY7cNe6H7gK/mf/wF4v5lZHccYtLLn7Jw75Jybyj99gVyztLjz87uGt3ruX6vn4ELi55wfBh5zzr0J4Jw7X+cxBs3POTtgWf7ntwGv1XF8oXDOPQe8UWKT+4EnXc4LwO1mdmc1x2rkgF62D3vhNi7Xc2YC6KzL6MLh55wL7SD3zR53gfXcjxE/v+tfA37NzJ43sxfM7L66jS4cfs75z4DtZnYW+CYwWJ+hRarS/+89+bljkTQgM9sO9AO/FfVYwlbQc/8jEQ+l3lrJpV02kvuX2HNm9i7n3KVIRxWuNLDfOfd/zOx9wFfNbL1zbibqgcVBI8/Qy/Vhn7ONmbWS+yfaxbqMLhx+zhkz+wCQIdem+HqdxhamZuy57+d3fRYYdc7ddM6dBH5GLsDHlZ9z3gF8DcA59z1gCbkGVknm6/97Pxo5oJfsw543Cvx+/ucHgWdd/ipDTJU9ZzN7N/C35IJ53HOqs5qx576fv9/PkJudY2YryKVgXq3nIAPm55zPAO8HMLM+cgE96ferHAU+nK92eS8w4Zw7V9Weor4CXObq8GZys5ITQCb/2mfJ/c8MuV/214FXgH8F3hn1mOtwzt8Bfgn8MP8YjXrM9TjvedseJuZVLj5/10Yu1fQycATYGvWY63DOa4HnyVXA/BD4najHHMA5jwDngJvk/tW1A3gEeKTg9/xY/r/JkVr+bmvpv4hIQjRyykVERCqggC4ikhAK6CIiCaGALiKSEAroIiIJoYAuIpIQCugiIgnx/wHyq955rpp86gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEolEpERm8GH",
        "outputId": "477578fb-4c53-4c7d-dd42-c2a1893ff1c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x_train = torch.Tensor(x) # แปลงอาเรย์เป็นเทนเซอร์\r\n",
        "y_train = torch.Tensor(y) # แปลงอาเรย์เป็นเทนเซอร์\r\n",
        "x_train = x_train.view(-1,1)\r\n",
        "y_train = y_train.view(-1,1)\r\n",
        "import torch\r\n",
        "from torch.autograd import Variable\r\n",
        "class linearRegression(torch.nn.Module):\r\n",
        "    def __init__(self, inputSize, outputSize):\r\n",
        "        super(linearRegression, self).__init__()\r\n",
        "        self.linear = torch.nn.Linear(inputSize, outputSize)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.linear(x)\r\n",
        "        return out\r\n",
        "\r\n",
        "inputDim = 1        # takes variable 'x' \r\n",
        "outputDim = 1       # takes variable 'y'\r\n",
        "learningRate = 0.01 \r\n",
        "epochs = 500\r\n",
        "\r\n",
        "processing = 'cpu'\r\n",
        "if torch.cuda.is_available():\r\n",
        "  processing = 'cuda'\r\n",
        "device = torch.device(processing)\r\n",
        "\r\n",
        "t = torch.ones(5,3).to(device)\r\n",
        "\r\n",
        "model = linearRegression(inputDim, outputDim)\r\n",
        "##### For GPU #######\r\n",
        "if torch.cuda.is_available():\r\n",
        "    model.cuda()\r\n",
        "criterion = torch.nn.MSELoss() \r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\r\n",
        "for epoch in range(epochs):\r\n",
        "    # Converting inputs and labels to Variable\r\n",
        "    inputs = x_train.to(device)\r\n",
        "    labels = y_train.to(device)\r\n",
        "\r\n",
        "    # get output from the model, given the inputs\r\n",
        "    outputs = model(inputs)\r\n",
        "\r\n",
        "    # get loss for the predicted output\r\n",
        "    loss = criterion(outputs, labels)\r\n",
        "    print(loss)\r\n",
        "    # get gradients w.r.t to parameters\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # update parameters\r\n",
        "    optimizer.step()\r\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\r\n",
        "    optimizer.zero_grad()\r\n",
        "    \r\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))\r\n",
        "\r\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.5555, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 0, loss 0.5554608702659607\n",
            "tensor(0.5340, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 1, loss 0.5340451002120972\n",
            "tensor(0.5136, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 2, loss 0.5136423110961914\n",
            "tensor(0.4942, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 3, loss 0.49420398473739624\n",
            "tensor(0.4757, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 4, loss 0.47568410634994507\n",
            "tensor(0.4580, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 5, loss 0.45803868770599365\n",
            "tensor(0.4412, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 6, loss 0.4412260055541992\n",
            "tensor(0.4252, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 7, loss 0.4252062141895294\n",
            "tensor(0.4099, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 8, loss 0.4099414050579071\n",
            "tensor(0.3954, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 9, loss 0.3953954875469208\n",
            "tensor(0.3815, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 10, loss 0.3815340995788574\n",
            "tensor(0.3683, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 11, loss 0.36832448840141296\n",
            "tensor(0.3557, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 12, loss 0.3557356297969818\n",
            "tensor(0.3437, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 13, loss 0.34373772144317627\n",
            "tensor(0.3323, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 14, loss 0.3323024809360504\n",
            "tensor(0.3214, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 15, loss 0.3214030861854553\n",
            "tensor(0.3110, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 16, loss 0.31101396679878235\n",
            "tensor(0.3011, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 17, loss 0.3011106550693512\n",
            "tensor(0.2917, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 18, loss 0.2916699945926666\n",
            "tensor(0.2827, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 19, loss 0.28266987204551697\n",
            "tensor(0.2741, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 20, loss 0.27408918738365173\n",
            "tensor(0.2659, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 21, loss 0.2659079432487488\n",
            "tensor(0.2581, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 22, loss 0.2581070363521576\n",
            "tensor(0.2507, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 23, loss 0.25066834688186646\n",
            "tensor(0.2436, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 24, loss 0.24357451498508453\n",
            "tensor(0.2368, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 25, loss 0.2368091195821762\n",
            "tensor(0.2304, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 26, loss 0.2303563803434372\n",
            "tensor(0.2242, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 27, loss 0.22420147061347961\n",
            "tensor(0.2183, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 28, loss 0.21833012998104095\n",
            "tensor(0.2127, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 29, loss 0.21272879838943481\n",
            "tensor(0.2074, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 30, loss 0.2073846012353897\n",
            "tensor(0.2023, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 31, loss 0.20228523015975952\n",
            "tensor(0.1974, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 32, loss 0.19741904735565186\n",
            "tensor(0.1928, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 33, loss 0.1927749067544937\n",
            "tensor(0.1883, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 34, loss 0.1883421689271927\n",
            "tensor(0.1841, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 35, loss 0.1841108351945877\n",
            "tensor(0.1801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 36, loss 0.18007121980190277\n",
            "tensor(0.1762, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 37, loss 0.17621421813964844\n",
            "tensor(0.1725, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 38, loss 0.17253105342388153\n",
            "tensor(0.1690, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 39, loss 0.16901350021362305\n",
            "tensor(0.1657, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 40, loss 0.16565360128879547\n",
            "tensor(0.1624, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 41, loss 0.16244390606880188\n",
            "tensor(0.1594, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 42, loss 0.15937720239162445\n",
            "tensor(0.1564, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 43, loss 0.1564466953277588\n",
            "tensor(0.1536, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 44, loss 0.15364591777324677\n",
            "tensor(0.1510, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 45, loss 0.15096865594387054\n",
            "tensor(0.1484, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 46, loss 0.14840900897979736\n",
            "tensor(0.1460, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 47, loss 0.14596140384674072\n",
            "tensor(0.1436, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 48, loss 0.14362049102783203\n",
            "tensor(0.1414, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 49, loss 0.14138120412826538\n",
            "tensor(0.1392, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 50, loss 0.13923868536949158\n",
            "tensor(0.1372, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 51, loss 0.1371883600950241\n",
            "tensor(0.1352, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 52, loss 0.1352258026599884\n",
            "tensor(0.1333, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 53, loss 0.13334685564041138\n",
            "tensor(0.1315, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 54, loss 0.13154755532741547\n",
            "tensor(0.1298, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 55, loss 0.12982408702373505\n",
            "tensor(0.1282, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 56, loss 0.1281728744506836\n",
            "tensor(0.1266, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 57, loss 0.12659047544002533\n",
            "tensor(0.1251, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 58, loss 0.1250736117362976\n",
            "tensor(0.1236, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 59, loss 0.1236191838979721\n",
            "tensor(0.1222, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 60, loss 0.12222420424222946\n",
            "tensor(0.1209, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 61, loss 0.12088589370250702\n",
            "tensor(0.1196, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 62, loss 0.1196015402674675\n",
            "tensor(0.1184, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 63, loss 0.11836858838796616\n",
            "tensor(0.1172, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 64, loss 0.1171845868229866\n",
            "tensor(0.1160, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 65, loss 0.11604724824428558\n",
            "tensor(0.1150, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 66, loss 0.11495433747768402\n",
            "tensor(0.1139, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 67, loss 0.11390374600887299\n",
            "tensor(0.1129, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 68, loss 0.1128934994339943\n",
            "tensor(0.1119, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 69, loss 0.11192169040441513\n",
            "tensor(0.1110, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 70, loss 0.11098647862672806\n",
            "tensor(0.1101, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 71, loss 0.11008613556623459\n",
            "tensor(0.1092, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 72, loss 0.10921904444694519\n",
            "tensor(0.1084, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 73, loss 0.10838361829519272\n",
            "tensor(0.1076, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 74, loss 0.10757836699485779\n",
            "tensor(0.1068, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 75, loss 0.10680186003446579\n",
            "tensor(0.1061, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 76, loss 0.10605275630950928\n",
            "tensor(0.1053, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 77, loss 0.10532976686954498\n",
            "tensor(0.1046, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 78, loss 0.10463166981935501\n",
            "tensor(0.1040, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 79, loss 0.10395728796720505\n",
            "tensor(0.1033, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 80, loss 0.10330551117658615\n",
            "tensor(0.1027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 81, loss 0.10267526656389236\n",
            "tensor(0.1021, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 82, loss 0.1020655632019043\n",
            "tensor(0.1015, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 83, loss 0.10147544741630554\n",
            "tensor(0.1009, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 84, loss 0.10090398788452148\n",
            "tensor(0.1004, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 85, loss 0.10035032033920288\n",
            "tensor(0.0998, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 86, loss 0.09981361031532288\n",
            "tensor(0.0993, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 87, loss 0.09929307550191879\n",
            "tensor(0.0988, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 88, loss 0.09878798574209213\n",
            "tensor(0.0983, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 89, loss 0.09829757362604141\n",
            "tensor(0.0978, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 90, loss 0.0978211835026741\n",
            "tensor(0.0974, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 91, loss 0.09735818952322006\n",
            "tensor(0.0969, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 92, loss 0.09690792113542557\n",
            "tensor(0.0965, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 93, loss 0.09646984189748764\n",
            "tensor(0.0960, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 94, loss 0.09604336321353912\n",
            "tensor(0.0956, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 95, loss 0.09562795609235764\n",
            "tensor(0.0952, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 96, loss 0.09522312134504318\n",
            "tensor(0.0948, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 97, loss 0.09482835233211517\n",
            "tensor(0.0944, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 98, loss 0.09444320201873779\n",
            "tensor(0.0941, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 99, loss 0.09406724572181702\n",
            "tensor(0.0937, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 100, loss 0.09370004385709763\n",
            "tensor(0.0933, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 101, loss 0.09334121644496918\n",
            "tensor(0.0930, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 102, loss 0.09299036860466003\n",
            "tensor(0.0926, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 103, loss 0.09264715015888214\n",
            "tensor(0.0923, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 104, loss 0.09231121093034744\n",
            "tensor(0.0920, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 105, loss 0.09198220819234848\n",
            "tensor(0.0917, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 106, loss 0.09165986627340317\n",
            "tensor(0.0913, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 107, loss 0.09134386479854584\n",
            "tensor(0.0910, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 108, loss 0.091033935546875\n",
            "tensor(0.0907, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 109, loss 0.09072978794574738\n",
            "tensor(0.0904, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 110, loss 0.09043117612600327\n",
            "tensor(0.0901, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 111, loss 0.09013786166906357\n",
            "tensor(0.0898, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 112, loss 0.08984960615634918\n",
            "tensor(0.0896, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 113, loss 0.0895661935210228\n",
            "tensor(0.0893, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 114, loss 0.0892874151468277\n",
            "tensor(0.0890, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 115, loss 0.08901304006576538\n",
            "tensor(0.0887, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 116, loss 0.08874291926622391\n",
            "tensor(0.0885, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 117, loss 0.08847684413194656\n",
            "tensor(0.0882, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 118, loss 0.08821466565132141\n",
            "tensor(0.0880, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 119, loss 0.08795617520809174\n",
            "tensor(0.0877, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 120, loss 0.08770126849412918\n",
            "tensor(0.0874, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 121, loss 0.08744975924491882\n",
            "tensor(0.0872, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 122, loss 0.08720151335000992\n",
            "tensor(0.0870, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 123, loss 0.08695639669895172\n",
            "tensor(0.0867, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 124, loss 0.08671428263187408\n",
            "tensor(0.0865, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 125, loss 0.08647502958774567\n",
            "tensor(0.0862, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 126, loss 0.0862385481595993\n",
            "tensor(0.0860, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 127, loss 0.08600471168756485\n",
            "tensor(0.0858, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 128, loss 0.08577340096235275\n",
            "tensor(0.0855, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 129, loss 0.08554454892873764\n",
            "tensor(0.0853, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 130, loss 0.08531802147626877\n",
            "tensor(0.0851, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 131, loss 0.08509375154972076\n",
            "tensor(0.0849, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 132, loss 0.08487164229154587\n",
            "tensor(0.0847, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 133, loss 0.08465161174535751\n",
            "tensor(0.0844, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 134, loss 0.08443357050418854\n",
            "tensor(0.0842, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 135, loss 0.08421745151281357\n",
            "tensor(0.0840, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 136, loss 0.08400319516658783\n",
            "tensor(0.0838, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 137, loss 0.08379070460796356\n",
            "tensor(0.0836, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 138, loss 0.08357994258403778\n",
            "tensor(0.0834, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 139, loss 0.08337080478668213\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 140, loss 0.08316327631473541\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 141, loss 0.08295728266239166\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 142, loss 0.0827527791261673\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 143, loss 0.08254970610141754\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 144, loss 0.08234800398349762\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 145, loss 0.08214763551950455\n",
            "tensor(0.0819, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 146, loss 0.08194856345653534\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 147, loss 0.08175073564052582\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 148, loss 0.0815541073679924\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 149, loss 0.0813586488366127\n",
            "tensor(0.0812, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 150, loss 0.08116433024406433\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 151, loss 0.08097109943628311\n",
            "tensor(0.0808, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 152, loss 0.08077891916036606\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 153, loss 0.08058778941631317\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 154, loss 0.08039764314889908\n",
            "tensor(0.0802, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 155, loss 0.08020846545696259\n",
            "tensor(0.0800, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 156, loss 0.0800202339887619\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 157, loss 0.07983292639255524\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 158, loss 0.07964649796485901\n",
            "tensor(0.0795, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 159, loss 0.07946094125509262\n",
            "tensor(0.0793, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 160, loss 0.07927622646093369\n",
            "tensor(0.0791, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 161, loss 0.07909233123064041\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 162, loss 0.0789092406630516\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 163, loss 0.07872693240642548\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 164, loss 0.07854538410902023\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 165, loss 0.07836458086967468\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 166, loss 0.07818450033664703\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 167, loss 0.07800512760877609\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 168, loss 0.07782646268606186\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 169, loss 0.07764846831560135\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 170, loss 0.07747112959623337\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 171, loss 0.07729445397853851\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 172, loss 0.0771184116601944\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 173, loss 0.07694298774003983\n",
            "tensor(0.0768, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 174, loss 0.0767681896686554\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 175, loss 0.07659398764371872\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 176, loss 0.0764203742146492\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 177, loss 0.07624734193086624\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 178, loss 0.07607488334178925\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 179, loss 0.07590298354625702\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 180, loss 0.07573163509368896\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 181, loss 0.07556083053350449\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 182, loss 0.07539056241512299\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 183, loss 0.07522081583738327\n",
            "tensor(0.0751, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 184, loss 0.07505159080028534\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 185, loss 0.074882872402668\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 186, loss 0.07471466809511185\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 187, loss 0.0745469480752945\n",
            "tensor(0.0744, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 188, loss 0.07437972724437714\n",
            "tensor(0.0742, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 189, loss 0.07421299815177917\n",
            "tensor(0.0740, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 190, loss 0.07404673099517822\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 191, loss 0.07388095557689667\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 192, loss 0.07371563464403152\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 193, loss 0.07355078309774399\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 194, loss 0.07338639348745346\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 195, loss 0.07322244346141815\n",
            "tensor(0.0731, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 196, loss 0.07305895537137985\n",
            "tensor(0.0729, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 197, loss 0.07289590686559677\n",
            "tensor(0.0727, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 198, loss 0.07273329049348831\n",
            "tensor(0.0726, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 199, loss 0.07257112115621567\n",
            "tensor(0.0724, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 200, loss 0.07240937650203705\n",
            "tensor(0.0722, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 201, loss 0.07224806398153305\n",
            "tensor(0.0721, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 202, loss 0.07208716869354248\n",
            "tensor(0.0719, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 203, loss 0.07192669808864594\n",
            "tensor(0.0718, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 204, loss 0.07176664471626282\n",
            "tensor(0.0716, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 205, loss 0.07160700857639313\n",
            "tensor(0.0714, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 206, loss 0.07144777476787567\n",
            "tensor(0.0713, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 207, loss 0.07128895074129105\n",
            "tensor(0.0711, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 208, loss 0.07113052904605865\n",
            "tensor(0.0710, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 209, loss 0.0709725171327591\n",
            "tensor(0.0708, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 210, loss 0.07081490010023117\n",
            "tensor(0.0707, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 211, loss 0.07065767794847488\n",
            "tensor(0.0705, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 212, loss 0.07050085812807083\n",
            "tensor(0.0703, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 213, loss 0.07034442573785782\n",
            "tensor(0.0702, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 214, loss 0.07018838822841644\n",
            "tensor(0.0700, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 215, loss 0.07003272324800491\n",
            "tensor(0.0699, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 216, loss 0.06987745314836502\n",
            "tensor(0.0697, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 217, loss 0.06972255557775497\n",
            "tensor(0.0696, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 218, loss 0.06956805288791656\n",
            "tensor(0.0694, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 219, loss 0.0694139152765274\n",
            "tensor(0.0693, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 220, loss 0.06926016509532928\n",
            "tensor(0.0691, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 221, loss 0.06910677999258041\n",
            "tensor(0.0690, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 222, loss 0.06895377486944199\n",
            "tensor(0.0688, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 223, loss 0.06880113482475281\n",
            "tensor(0.0686, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 224, loss 0.06864886730909348\n",
            "tensor(0.0685, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 225, loss 0.06849697232246399\n",
            "tensor(0.0683, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 226, loss 0.06834542751312256\n",
            "tensor(0.0682, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 227, loss 0.06819426268339157\n",
            "tensor(0.0680, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 228, loss 0.06804344803094864\n",
            "tensor(0.0679, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 229, loss 0.06789299845695496\n",
            "tensor(0.0677, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 230, loss 0.06774290651082993\n",
            "tensor(0.0676, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 231, loss 0.06759317964315414\n",
            "tensor(0.0674, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 232, loss 0.06744381040334702\n",
            "tensor(0.0673, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 233, loss 0.06729478389024734\n",
            "tensor(0.0671, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 234, loss 0.06714611500501633\n",
            "tensor(0.0670, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 235, loss 0.06699780374765396\n",
            "tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 236, loss 0.06684984266757965\n",
            "tensor(0.0667, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 237, loss 0.0667022317647934\n",
            "tensor(0.0666, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 238, loss 0.0665549635887146\n",
            "tensor(0.0664, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 239, loss 0.06640803813934326\n",
            "tensor(0.0663, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 240, loss 0.06626147776842117\n",
            "tensor(0.0661, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 241, loss 0.06611524522304535\n",
            "tensor(0.0660, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 242, loss 0.06596936285495758\n",
            "tensor(0.0658, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 243, loss 0.06582382321357727\n",
            "tensor(0.0657, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 244, loss 0.06567862629890442\n",
            "tensor(0.0655, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 245, loss 0.06553376466035843\n",
            "tensor(0.0654, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 246, loss 0.0653892457485199\n",
            "tensor(0.0652, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 247, loss 0.06524506211280823\n",
            "tensor(0.0651, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 248, loss 0.06510121375322342\n",
            "tensor(0.0650, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 249, loss 0.06495770066976547\n",
            "tensor(0.0648, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 250, loss 0.06481453031301498\n",
            "tensor(0.0647, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 251, loss 0.06467168778181076\n",
            "tensor(0.0645, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 252, loss 0.064529187977314\n",
            "tensor(0.0644, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 253, loss 0.0643870085477829\n",
            "tensor(0.0642, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 254, loss 0.06424516439437866\n",
            "tensor(0.0641, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 255, loss 0.06410364806652069\n",
            "tensor(0.0640, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 256, loss 0.06396246701478958\n",
            "tensor(0.0638, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 257, loss 0.06382160633802414\n",
            "tensor(0.0637, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 258, loss 0.06368107348680496\n",
            "tensor(0.0635, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 259, loss 0.06354087591171265\n",
            "tensor(0.0634, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 260, loss 0.0634009912610054\n",
            "tensor(0.0633, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 261, loss 0.06326144188642502\n",
            "tensor(0.0631, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 262, loss 0.0631222128868103\n",
            "tensor(0.0630, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 263, loss 0.06298330426216125\n",
            "tensor(0.0628, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 264, loss 0.06284472346305847\n",
            "tensor(0.0627, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 265, loss 0.06270645558834076\n",
            "tensor(0.0626, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 266, loss 0.06256850808858871\n",
            "tensor(0.0624, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 267, loss 0.06243089213967323\n",
            "tensor(0.0623, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 268, loss 0.062293581664562225\n",
            "tensor(0.0622, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 269, loss 0.062156591564416885\n",
            "tensor(0.0620, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 270, loss 0.062019918113946915\n",
            "tensor(0.0619, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 271, loss 0.06188356131315231\n",
            "tensor(0.0617, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 272, loss 0.061747532337903976\n",
            "tensor(0.0616, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 273, loss 0.06161179766058922\n",
            "tensor(0.0615, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 274, loss 0.061476387083530426\n",
            "tensor(0.0613, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 275, loss 0.061341285705566406\n",
            "tensor(0.0612, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 276, loss 0.061206500977277756\n",
            "tensor(0.0611, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 277, loss 0.06107202544808388\n",
            "tensor(0.0609, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 278, loss 0.06093785539269447\n",
            "tensor(0.0608, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 279, loss 0.06080399826169014\n",
            "tensor(0.0607, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 280, loss 0.06067045032978058\n",
            "tensor(0.0605, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 281, loss 0.06053721159696579\n",
            "tensor(0.0604, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 282, loss 0.06040428206324577\n",
            "tensor(0.0603, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 283, loss 0.06027165427803993\n",
            "tensor(0.0601, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 284, loss 0.060139331966638565\n",
            "tensor(0.0600, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 285, loss 0.06000731885433197\n",
            "tensor(0.0599, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 286, loss 0.05987560749053955\n",
            "tensor(0.0597, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 287, loss 0.05974419787526131\n",
            "tensor(0.0596, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 288, loss 0.059613097459077835\n",
            "tensor(0.0595, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 289, loss 0.05948229506611824\n",
            "tensor(0.0594, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 290, loss 0.05935179442167282\n",
            "tensor(0.0592, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 291, loss 0.05922159552574158\n",
            "tensor(0.0591, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 292, loss 0.05909169465303421\n",
            "tensor(0.0590, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 293, loss 0.05896209552884102\n",
            "tensor(0.0588, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 294, loss 0.058832798153162\n",
            "tensor(0.0587, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 295, loss 0.05870378762483597\n",
            "tensor(0.0586, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 296, loss 0.05857507884502411\n",
            "tensor(0.0584, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 297, loss 0.058446671813726425\n",
            "tensor(0.0583, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 298, loss 0.05831855535507202\n",
            "tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 299, loss 0.058190736919641495\n",
            "tensor(0.0581, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 300, loss 0.05806320905685425\n",
            "tensor(0.0579, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 301, loss 0.05793597921729088\n",
            "tensor(0.0578, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 302, loss 0.05780903622508049\n",
            "tensor(0.0577, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 303, loss 0.05768238753080368\n",
            "tensor(0.0576, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 304, loss 0.05755603685975075\n",
            "tensor(0.0574, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 305, loss 0.0574299693107605\n",
            "tensor(0.0573, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 306, loss 0.057304199784994125\n",
            "tensor(0.0572, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 307, loss 0.057178717106580734\n",
            "tensor(0.0571, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 308, loss 0.057053517550230026\n",
            "tensor(0.0569, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 309, loss 0.0569286048412323\n",
            "tensor(0.0568, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 310, loss 0.05680398643016815\n",
            "tensor(0.0567, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 311, loss 0.05667965114116669\n",
            "tensor(0.0566, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 312, loss 0.056555602699518204\n",
            "tensor(0.0564, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 313, loss 0.0564318411052227\n",
            "tensor(0.0563, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 314, loss 0.05630836263298988\n",
            "tensor(0.0562, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 315, loss 0.05618516728281975\n",
            "tensor(0.0561, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 316, loss 0.05606226250529289\n",
            "tensor(0.0559, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 317, loss 0.055939629673957825\n",
            "tensor(0.0558, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 318, loss 0.05581727996468544\n",
            "tensor(0.0557, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 319, loss 0.05569521710276604\n",
            "tensor(0.0556, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 320, loss 0.05557343363761902\n",
            "tensor(0.0555, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 321, loss 0.05545193701982498\n",
            "tensor(0.0553, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 322, loss 0.055330708622932434\n",
            "tensor(0.0552, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 323, loss 0.05520976334810257\n",
            "tensor(0.0551, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 324, loss 0.05508910119533539\n",
            "tensor(0.0550, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 325, loss 0.054968707263469696\n",
            "tensor(0.0548, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 326, loss 0.05484859272837639\n",
            "tensor(0.0547, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 327, loss 0.05472875386476517\n",
            "tensor(0.0546, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 328, loss 0.05460920184850693\n",
            "tensor(0.0545, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 329, loss 0.05448991060256958\n",
            "tensor(0.0544, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 330, loss 0.054370902478694916\n",
            "tensor(0.0543, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 331, loss 0.05425216630101204\n",
            "tensor(0.0541, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 332, loss 0.05413369834423065\n",
            "tensor(0.0540, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 333, loss 0.05401550233364105\n",
            "tensor(0.0539, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 334, loss 0.05389758571982384\n",
            "tensor(0.0538, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 335, loss 0.05377994105219841\n",
            "tensor(0.0537, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 336, loss 0.053662557154893875\n",
            "tensor(0.0535, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 337, loss 0.05354544520378113\n",
            "tensor(0.0534, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 338, loss 0.05342860519886017\n",
            "tensor(0.0533, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 339, loss 0.0533120334148407\n",
            "tensor(0.0532, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 340, loss 0.053195733577013016\n",
            "tensor(0.0531, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 341, loss 0.053079698234796524\n",
            "tensor(0.0530, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 342, loss 0.05296392738819122\n",
            "tensor(0.0528, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 343, loss 0.05284842848777771\n",
            "tensor(0.0527, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 344, loss 0.05273319035768509\n",
            "tensor(0.0526, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 345, loss 0.05261821672320366\n",
            "tensor(0.0525, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 346, loss 0.05250350758433342\n",
            "tensor(0.0524, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 347, loss 0.05238906666636467\n",
            "tensor(0.0523, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 348, loss 0.052274882793426514\n",
            "tensor(0.0522, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 349, loss 0.05216096714138985\n",
            "tensor(0.0520, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 350, loss 0.052047308534383774\n",
            "tensor(0.0519, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 351, loss 0.05193391814827919\n",
            "tensor(0.0518, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 352, loss 0.0518207810819149\n",
            "tensor(0.0517, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 353, loss 0.051707904785871506\n",
            "tensor(0.0516, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 354, loss 0.0515952967107296\n",
            "tensor(0.0515, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 355, loss 0.05148293450474739\n",
            "tensor(0.0514, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 356, loss 0.05137084051966667\n",
            "tensor(0.0513, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 357, loss 0.05125899985432625\n",
            "tensor(0.0511, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 358, loss 0.05114741623401642\n",
            "tensor(0.0510, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 359, loss 0.051036085933446884\n",
            "tensor(0.0509, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 360, loss 0.05092501640319824\n",
            "tensor(0.0508, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 361, loss 0.050814203917980194\n",
            "tensor(0.0507, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 362, loss 0.050703637301921844\n",
            "tensor(0.0506, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 363, loss 0.050593335181474686\n",
            "tensor(0.0505, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 364, loss 0.050483282655477524\n",
            "tensor(0.0504, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 365, loss 0.050373487174510956\n",
            "tensor(0.0503, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 366, loss 0.050263937562704086\n",
            "tensor(0.0502, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 367, loss 0.05015464499592781\n",
            "tensor(0.0500, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 368, loss 0.05004560202360153\n",
            "tensor(0.0499, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 369, loss 0.04993680864572525\n",
            "tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 370, loss 0.04982826113700867\n",
            "tensor(0.0497, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 371, loss 0.04971997067332268\n",
            "tensor(0.0496, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 372, loss 0.04961192607879639\n",
            "tensor(0.0495, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 373, loss 0.049504127353429794\n",
            "tensor(0.0494, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 374, loss 0.0493965819478035\n",
            "tensor(0.0493, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 375, loss 0.0492892786860466\n",
            "tensor(0.0492, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 376, loss 0.0491822250187397\n",
            "tensor(0.0491, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 377, loss 0.0490754134953022\n",
            "tensor(0.0490, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 378, loss 0.0489688478410244\n",
            "tensor(0.0489, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 379, loss 0.048862531781196594\n",
            "tensor(0.0488, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 380, loss 0.04875645413994789\n",
            "tensor(0.0487, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 381, loss 0.048650626093149185\n",
            "tensor(0.0485, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 382, loss 0.04854504019021988\n",
            "tensor(0.0484, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 383, loss 0.048439692705869675\n",
            "tensor(0.0483, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 384, loss 0.04833458736538887\n",
            "tensor(0.0482, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 385, loss 0.04822973161935806\n",
            "tensor(0.0481, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 386, loss 0.04812511429190636\n",
            "tensor(0.0480, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 387, loss 0.04802073538303375\n",
            "tensor(0.0479, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 388, loss 0.04791659116744995\n",
            "tensor(0.0478, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 389, loss 0.04781269282102585\n",
            "tensor(0.0477, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 390, loss 0.047709036618471146\n",
            "tensor(0.0476, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 391, loss 0.04760560765862465\n",
            "tensor(0.0475, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 392, loss 0.04750242456793785\n",
            "tensor(0.0474, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 393, loss 0.047399479895830154\n",
            "tensor(0.0473, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 394, loss 0.04729676619172096\n",
            "tensor(0.0472, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 395, loss 0.04719429463148117\n",
            "tensor(0.0471, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 396, loss 0.04709205403923988\n",
            "tensor(0.0470, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 397, loss 0.0469900518655777\n",
            "tensor(0.0469, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 398, loss 0.046888284385204315\n",
            "tensor(0.0468, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 399, loss 0.04678674414753914\n",
            "tensor(0.0467, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 400, loss 0.04668544605374336\n",
            "tensor(0.0466, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 401, loss 0.04658437520265579\n",
            "tensor(0.0465, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 402, loss 0.046483542770147324\n",
            "tensor(0.0464, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 403, loss 0.04638293758034706\n",
            "tensor(0.0463, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 404, loss 0.0462825633585453\n",
            "tensor(0.0462, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 405, loss 0.04618242010474205\n",
            "tensor(0.0461, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 406, loss 0.046082504093647\n",
            "tensor(0.0460, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 407, loss 0.04598282277584076\n",
            "tensor(0.0459, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 408, loss 0.04588336497545242\n",
            "tensor(0.0458, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 409, loss 0.04578413814306259\n",
            "tensor(0.0457, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 410, loss 0.045685142278671265\n",
            "tensor(0.0456, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 411, loss 0.045586373656988144\n",
            "tensor(0.0455, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 412, loss 0.04548783227801323\n",
            "tensor(0.0454, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 413, loss 0.04538951441645622\n",
            "tensor(0.0453, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 414, loss 0.04529142379760742\n",
            "tensor(0.0452, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 415, loss 0.04519356042146683\n",
            "tensor(0.0451, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 416, loss 0.04509592056274414\n",
            "tensor(0.0450, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 417, loss 0.04499850049614906\n",
            "tensor(0.0449, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 418, loss 0.04490131139755249\n",
            "tensor(0.0448, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 419, loss 0.044804349541664124\n",
            "tensor(0.0447, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 420, loss 0.044707607477903366\n",
            "tensor(0.0446, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 421, loss 0.04461108148097992\n",
            "tensor(0.0445, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 422, loss 0.04451478272676468\n",
            "tensor(0.0444, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 423, loss 0.044418707489967346\n",
            "tensor(0.0443, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 424, loss 0.044322848320007324\n",
            "tensor(0.0442, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 425, loss 0.04422720894217491\n",
            "tensor(0.0441, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 426, loss 0.044131793081760406\n",
            "tensor(0.0440, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 427, loss 0.04403659328818321\n",
            "tensor(0.0439, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 428, loss 0.043941617012023926\n",
            "tensor(0.0438, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 429, loss 0.04384685307741165\n",
            "tensor(0.0438, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 430, loss 0.04375230520963669\n",
            "tensor(0.0437, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 431, loss 0.04365798458456993\n",
            "tensor(0.0436, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 432, loss 0.043563876301050186\n",
            "tensor(0.0435, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 433, loss 0.043469980359077454\n",
            "tensor(0.0434, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 434, loss 0.04337630793452263\n",
            "tensor(0.0433, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 435, loss 0.04328284040093422\n",
            "tensor(0.0432, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 436, loss 0.04318959638476372\n",
            "tensor(0.0431, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 437, loss 0.04309656471014023\n",
            "tensor(0.0430, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 438, loss 0.04300374910235405\n",
            "tensor(0.0429, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 439, loss 0.04291114583611488\n",
            "tensor(0.0428, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 440, loss 0.04281875118613243\n",
            "tensor(0.0427, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 441, loss 0.04272657260298729\n",
            "tensor(0.0426, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 442, loss 0.04263459891080856\n",
            "tensor(0.0425, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 443, loss 0.042542848736047745\n",
            "tensor(0.0425, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 444, loss 0.04245130345225334\n",
            "tensor(0.0424, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 445, loss 0.04235997051000595\n",
            "tensor(0.0423, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 446, loss 0.042268842458724976\n",
            "tensor(0.0422, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 447, loss 0.042177923023700714\n",
            "tensor(0.0421, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 448, loss 0.04208722338080406\n",
            "tensor(0.0420, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 449, loss 0.04199672490358353\n",
            "tensor(0.0419, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 450, loss 0.041906438767910004\n",
            "tensor(0.0418, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 451, loss 0.041816346347332\n",
            "tensor(0.0417, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 452, loss 0.04172647371888161\n",
            "tensor(0.0416, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 453, loss 0.04163680598139763\n",
            "tensor(0.0415, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 454, loss 0.041547346860170364\n",
            "tensor(0.0415, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 455, loss 0.04145808517932892\n",
            "tensor(0.0414, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 456, loss 0.041369035840034485\n",
            "tensor(0.0413, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 457, loss 0.04128018766641617\n",
            "tensor(0.0412, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 458, loss 0.04119154438376427\n",
            "tensor(0.0411, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 459, loss 0.04110310971736908\n",
            "tensor(0.0410, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 460, loss 0.04101487249135971\n",
            "tensor(0.0409, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 461, loss 0.04092683643102646\n",
            "tensor(0.0408, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 462, loss 0.04083900898694992\n",
            "tensor(0.0408, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 463, loss 0.0407513752579689\n",
            "tensor(0.0407, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 464, loss 0.0406639501452446\n",
            "tensor(0.0406, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 465, loss 0.04057672247290611\n",
            "tensor(0.0405, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 466, loss 0.04048969969153404\n",
            "tensor(0.0404, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 467, loss 0.04040287062525749\n",
            "tensor(0.0403, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 468, loss 0.04031624644994736\n",
            "tensor(0.0402, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 469, loss 0.04022981971502304\n",
            "tensor(0.0401, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 470, loss 0.04014359414577484\n",
            "tensor(0.0401, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 471, loss 0.04005756229162216\n",
            "tensor(0.0400, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 472, loss 0.0399717316031456\n",
            "tensor(0.0399, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 473, loss 0.03988609462976456\n",
            "tensor(0.0398, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 474, loss 0.03980065882205963\n",
            "tensor(0.0397, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 475, loss 0.039715416729450226\n",
            "tensor(0.0396, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 476, loss 0.03963036835193634\n",
            "tensor(0.0395, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 477, loss 0.03954552114009857\n",
            "tensor(0.0395, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 478, loss 0.039460863918066025\n",
            "tensor(0.0394, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 479, loss 0.039376404136419296\n",
            "tensor(0.0393, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 480, loss 0.03929213806986809\n",
            "tensor(0.0392, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 481, loss 0.0392080657184124\n",
            "tensor(0.0391, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 482, loss 0.03912418335676193\n",
            "tensor(0.0390, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 483, loss 0.039040498435497284\n",
            "tensor(0.0390, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 484, loss 0.038957007229328156\n",
            "tensor(0.0389, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 485, loss 0.03887370601296425\n",
            "tensor(0.0388, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 486, loss 0.03879059851169586\n",
            "tensor(0.0387, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 487, loss 0.0387076735496521\n",
            "tensor(0.0386, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 488, loss 0.038624949753284454\n",
            "tensor(0.0385, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 489, loss 0.038542404770851135\n",
            "tensor(0.0385, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 490, loss 0.038460057228803635\n",
            "tensor(0.0384, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 491, loss 0.038377899676561356\n",
            "tensor(0.0383, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 492, loss 0.0382959246635437\n",
            "tensor(0.0382, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 493, loss 0.03821414336562157\n",
            "tensor(0.0381, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 494, loss 0.038132548332214355\n",
            "tensor(0.0381, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 495, loss 0.038051143288612366\n",
            "tensor(0.0380, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 496, loss 0.037969920784235\n",
            "tensor(0.0379, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 497, loss 0.03788888826966286\n",
            "tensor(0.0378, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 498, loss 0.037808045744895935\n",
            "tensor(0.0377, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "epoch 499, loss 0.03772738203406334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPd4c9pLm-hv",
        "outputId": "d6d45846-548c-4f74-c657-90950e022cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "with torch.no_grad(): # we don't need gradients in the testing phase\r\n",
        "        predicted = model(x_train.to(device)).cpu()\r\n",
        "\r\n",
        "plt.clf()\r\n",
        "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\r\n",
        "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\r\n",
        "plt.legend(loc='best')\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}